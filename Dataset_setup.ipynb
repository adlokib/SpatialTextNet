{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0beb79a8",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "084a05fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,zipfile\n",
    "import tarfile,json\n",
    "import imagesize\n",
    "import numpy as np\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from utils.args import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88aa67c7",
   "metadata": {},
   "source": [
    "# Raw data and processed data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954e786f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for the zips and json of RRC data\n",
    "zip_path = 'rrc_data/zips/'\n",
    "json_path = 'rrc_data/json/'\n",
    "\n",
    "# Path where the zips are supposed to be extracted to\n",
    "zip_dest = 'rrc_data/images/'\n",
    "\n",
    "# Path where the txt files and symlinks to images will be created\n",
    "train_path = 'data/train/'\n",
    "val_path = 'data/val/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "49a30f33",
   "metadata": {},
   "source": [
    "# Extracting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d40c5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These files are supposed to be downloaded from the RRC website and placed in the proper folder\n",
    "# https://rrc.cvc.uab.es/?ch=19&com=downloads\n",
    "\n",
    "with zipfile.ZipFile(os.path.join(zip_path,'val.zip'), 'r') as file:\n",
    "    file.extractall(zip_dest)\n",
    "    \n",
    "with zipfile.ZipFile(os.path.join(zip_path,'test_croppedv2.zip'), 'r') as file:\n",
    "    file.extractall(zip_dest)\n",
    "    \n",
    "with tarfile.open(os.path.join(zip_path,'cropped_train.tar.gz')) as file:\n",
    "    file.extractall(zip_dest)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2445218e",
   "metadata": {},
   "source": [
    "# Loading Transcriptions from json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65536527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These files are supposed to be downloaded from the RRC website and placed in the proper folder\n",
    "# https://rrc.cvc.uab.es/?ch=19&com=downloads\n",
    "\n",
    "with open(os.path.join(json_path,'cropped_train_v1.json'))as f:\n",
    "    data_train = json.load(f)\n",
    "    \n",
    "with open(os.path.join(json_path,'cropped_val_v1.json'))as f:\n",
    "    data_val = json.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47733eaf",
   "metadata": {},
   "source": [
    "# Creating dataset and writing transcriptions to txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a431946",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(val_path,exist_ok=True)\n",
    "os.makedirs(train_path,exist_ok=True)\n",
    "\n",
    "for entry in tqdm(data_val):\n",
    "    \n",
    "    img_file = str(entry['text_id'])+'.jpg'\n",
    "    \n",
    "    # Consider if the string is made of ASCII characters, if the image file is not empty, and finally exclude entries where NULL, '\\n' and '\\t' are present\n",
    "    if entry['transcription'].isascii() and os.path.getsize(os.path.join(zip_dest,'val',img_file)) and all([x not in entry['transcription'] for x in exclude_ascii ]):\n",
    "        \n",
    "        # Symlink to image to avoid unnecessary data copy\n",
    "        os.symlink(os.path.realpath(os.path.join(zip_dest,'val',img_file)),os.path.join(val_path,img_file))\n",
    "        \n",
    "        # Store transcription in txt file\n",
    "        with open(os.path.join(val_path,img_file[:-3]+'txt'),'w')as f:\n",
    "            f.write(entry['transcription'])\n",
    "            \n",
    "\n",
    "\n",
    "for entry in tqdm(data_train):\n",
    "    \n",
    "    img_file = str(entry['text_id'])+'.jpg'\n",
    "    \n",
    "    # Consider if the string is made of ASCII characters, if the image file is not empty, and finally exclude entries where NULL, '\\n' and '\\t' are present\n",
    "    if entry['transcription'].isascii() and os.path.getsize(os.path.join(zip_dest,'train',img_file)) and all([x not in entry['transcription'] for x in exclude_ascii ]):\n",
    "        \n",
    "        # Symlink to image to avoid unnecessary data copy\n",
    "        os.symlink(os.path.realpath(os.path.join(zip_dest,'train',img_file)),os.path.join(train_path,img_file))\n",
    "        \n",
    "        # Store transcription in txt file\n",
    "        with open(os.path.join(train_path,img_file[:-3]+'txt'),'w')as f:\n",
    "            f.write(entry['transcription'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ce03c14",
   "metadata": {},
   "source": [
    "# Delete one problematic file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4958fd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This image has some problem, no EOS character. So manually remove\n",
    "os.remove(os.path.join(train_path,'4985023.jpg'))\n",
    "os.remove(os.path.join(train_path,'4985023.txt'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6841b0df",
   "metadata": {},
   "source": [
    "# Read all txt files in val and train to further clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7724400",
   "metadata": {},
   "outputs": [],
   "source": [
    "txtl_val = glob.glob(os.path.join(val_path,'*.txt'))\n",
    "txtl_train = glob.glob(os.path.join(train_path,'*.txt'))\n",
    "\n",
    "# Convert to numpy array for easy manipulation\n",
    "all_files = np.array(txtl_val+txtl_train)\n",
    "\n",
    "txtl_val = np.array(txtl_val)\n",
    "txtl_train = np.array(txtl_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5509b35",
   "metadata": {},
   "source": [
    "# Query all the image dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa87c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the images and store width, height and w/h ratio\n",
    "w_l, h_l, rat_l = [], [], []\n",
    "\n",
    "for file in tqdm(all_files):\n",
    "    width, height = imagesize.get(file[:-3]+'jpg')\n",
    "    w_l.append(width)\n",
    "    h_l.append(height)\n",
    "    rat_l.append(width/height)\n",
    "    \n",
    "w_l, h_l, rat_l = np.array(w_l), np.array(h_l), np.array(rat_l)\n",
    "\n",
    "# take inverse of w/h ratio to get h/w ratio as well\n",
    "inv_l = 1/rat_l"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b077304",
   "metadata": {},
   "source": [
    "# Query all the transcription lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5efd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_len = []\n",
    "for file in tqdm(all_files):\n",
    "    with open(file)as f:\n",
    "        line = f.readline()\n",
    "    \n",
    "    str_len.append(len(line))\n",
    "    \n",
    "str_len = np.array(str_len)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4310faf0",
   "metadata": {},
   "source": [
    "# Filter the files based on cutoff criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d611feb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter based on max h/w ratio, max w/h ratio, max height, max width, min height, min width and max string length\n",
    "\n",
    "remove_bool = ((inv_l>max_HbyW) | (rat_l>max_WbyH) | (h_l>max_height) | (w_l>max_width) | (h_l<min_height) | (w_l<min_width) | (str_len>max_str_len))\n",
    "remove_files = all_files[remove_bool]\n",
    "\n",
    "# remove selected files\n",
    "for file in remove_files:\n",
    "    os.remove(file)\n",
    "    os.remove(file[:-3]+'jpg')\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "091bcce6",
   "metadata": {},
   "source": [
    "# Separate dataset into Tall, Square, and Wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42ecd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_len = len(txtl_val)\n",
    "\n",
    "val_remove_bool = remove_bool[:val_len]\n",
    "train_remove_bool = remove_bool[val_len:]\n",
    "\n",
    "# Files are separated into, tall, square and wide\n",
    "tall = rat_l < (1/max_HbyW_for_sq)\n",
    "square = (rat_l <= (max_WbyH_for_sq)) & (rat_l >= (1/max_HbyW_for_sq))\n",
    "wide = rat_l > (max_WbyH_for_sq)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b2b9ab2",
   "metadata": {},
   "source": [
    "# Filenames for each category extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030cb56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files are considered if they fit their resolution criteria and have not been filtered\n",
    "tall_val_files = txtl_val[((tall[:val_len]) & (~val_remove_bool))]\n",
    "square_val_files = txtl_val[((square[:val_len]) & (~val_remove_bool))]\n",
    "wide_val_files = txtl_val[((wide[:val_len]) & (~val_remove_bool))]\n",
    "\n",
    "tall_train_files = txtl_train[((tall[val_len:]) & (~train_remove_bool))]\n",
    "square_train_files = txtl_train[((square[val_len:]) & (~train_remove_bool))]\n",
    "wide_train_files = txtl_train[((wide[val_len:]) & (~train_remove_bool))]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f833635",
   "metadata": {},
   "source": [
    "# Create dictionary which will be used at train time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f815b1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dict = {'tall':[x.split(os.sep)[-1] for x in tall_val_files],\n",
    "            'square':[x.split(os.sep)[-1] for x in square_val_files],\n",
    "            'wide':[x.split(os.sep)[-1] for x in wide_val_files]}\n",
    "\n",
    "train_dict = {'tall':[x.split(os.sep)[-1] for x in tall_train_files],\n",
    "              'square':[x.split(os.sep)[-1] for x in square_train_files],\n",
    "              'wide':[x.split(os.sep)[-1] for x in wide_train_files]}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ea1d5b34",
   "metadata": {},
   "source": [
    "# Save dictionaries to pkl file for easy access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2859ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('val_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(val_dict, f)\n",
    "    \n",
    "with open('train_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(train_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
