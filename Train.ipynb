{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "96fce99f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319026d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,cv2,torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "from torch import Tensor\n",
    "from functools import partial\n",
    "\n",
    "from utils.args import *\n",
    "from utils.arch import *\n",
    "from utils.helper import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c71cdd8",
   "metadata": {},
   "source": [
    "# Dataset path and Pickle files created by Dataset_setup notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c99bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'data/train/'\n",
    "val_path = 'data/val/'\n",
    "\n",
    "with open('train_dict.pkl', 'rb') as f:\n",
    "    train_dict = pickle.load(f)\n",
    "\n",
    "with open('val_dict.pkl', 'rb') as f:\n",
    "    val_dict = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1a455d2",
   "metadata": {},
   "source": [
    "# Train and Val datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef963cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = {'tall':None,'square':None,'wide':None}\n",
    "val_dataset = {'tall':None,'square':None,'wide':None}\n",
    "\n",
    "for res in train_dataset.keys():\n",
    "    train_dataset[res] = [(os.path.join(train_path,x[:-3]+'jpg'),os.path.join(train_path,x)) for x in train_dict[res]]\n",
    "    \n",
    "for res in val_dataset.keys():\n",
    "    val_dataset[res] = [(os.path.join(val_path,x[:-3]+'jpg'),os.path.join(val_path,x)) for x in val_dict[res]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42fcaff6",
   "metadata": {},
   "source": [
    "# Train and Val dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa6b551",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loaders = {}\n",
    "\n",
    "train_loaders['tall'] = DataLoader(train_dataset['tall'], batch_size=BATCH_SIZE,shuffle=True, collate_fn=partial(generate_batch, w=tall_res[1], h=tall_res[0]))\n",
    "train_loaders['square'] = DataLoader(train_dataset['square'], batch_size=BATCH_SIZE,shuffle=True, collate_fn=partial(generate_batch, w=square_res[1], h=square_res[0]))\n",
    "train_loaders['wide'] = DataLoader(train_dataset['wide'], batch_size=BATCH_SIZE,shuffle=True, collate_fn=partial(generate_batch, w=wide_res[1], h=wide_res[0]))\n",
    "\n",
    "val_loaders = {}\n",
    "\n",
    "val_loaders['tall'] = DataLoader(val_dataset['tall'], batch_size=BATCH_SIZE,shuffle=True, collate_fn=partial(generate_batch, w=tall_res[1], h=tall_res[0]))\n",
    "val_loaders['square'] = DataLoader(val_dataset['square'], batch_size=BATCH_SIZE,shuffle=True, collate_fn=partial(generate_batch, w=square_res[1], h=square_res[0]))\n",
    "val_loaders['wide'] = DataLoader(val_dataset['wide'], batch_size=BATCH_SIZE,shuffle=True, collate_fn=partial(generate_batch, w=wide_res[1], h=wide_res[0]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e95370c7",
   "metadata": {},
   "source": [
    "# Train and val dataloader length info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea9bf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lens = {}\n",
    "for res in train_loaders.keys():\n",
    "    train_lens[res] = len(train_loaders[res])\n",
    "    \n",
    "val_lens = {}\n",
    "for res in val_loaders.keys():\n",
    "    val_lens[res] = len(val_loaders[res])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae7c5105",
   "metadata": {},
   "source": [
    "# Network, loss function and optimizer initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b26df89",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network(DEVICE).to(DEVICE)\n",
    "\n",
    "## Code to load intermediate weights and continue training\n",
    "# model_path = '<model_checkpoint>.pth'\n",
    "# dic=torch.load(model_path,map_location=torch.device(DEVICE))\n",
    "# net.load_state_dict(dic)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=1)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f7a9467",
   "metadata": {},
   "source": [
    "# Image augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c99a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "T=torchvision.transforms.RandomChoice([\n",
    "    torchvision.transforms.ColorJitter(0.85,0.85,0.85,0.5),\n",
    "    torchvision.transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5))\n",
    "    ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bdb29ae6",
   "metadata": {},
   "source": [
    "# Initial iteration to figure out max batch size that can be utilized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca3f5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs,tgt_pad,targets = generate_batch(train_dataset['wide'][11*BATCH_SIZE:12*BATCH_SIZE],w=wide_res[1], h=wide_res[0])\n",
    "imgs = imgs.to(DEVICE)\n",
    "targets = targets.to(DEVICE)\n",
    "tgt_pad = tgt_pad.to(DEVICE)\n",
    "\n",
    "imgs = torch.stack([T(x) for x in imgs])\n",
    "\n",
    "logits = net(imgs/255,targets[:-1,:],tgt_pad)\n",
    "\n",
    "targets = targets[1:].reshape(-1)\n",
    "logits = logits.reshape(-1, logits.shape[-1])\n",
    "\n",
    "loss = loss_fn(logits,targets)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "606e9e06",
   "metadata": {},
   "source": [
    "# Main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8232df33",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_val_loss = 9999\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    \n",
    "    losses = 0.0\n",
    "    val_losses = 0.0\n",
    "\n",
    "    print(\"Starting train for \", epoch)\n",
    "    \n",
    "    net = net.train()\n",
    "    for p in net.parameters():\n",
    "        p.requires_grad = True\n",
    "    \n",
    "    done_iters = {'tall':False,'square':False,'wide':False}\n",
    "    \n",
    "    train_iters = {}\n",
    "    for res in train_loaders.keys():\n",
    "        train_iters[res] = enumerate(train_loaders[res])\n",
    "    \n",
    "    while not all(done_iters.values()):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        sel = np.random.choice(['tall','square','wide'],p=resolution_dist)\n",
    "        while done_iters[sel]:\n",
    "            sel = np.random.choice(['tall','square','wide'],p=resolution_dist)\n",
    "        \n",
    "        i,(imgs,tgt_pad,targets) = next(train_iters[sel])\n",
    "        \n",
    "        if i == train_lens[sel]-1:\n",
    "            done_iters[sel] = True\n",
    "            \n",
    "        imgs = imgs.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "        tgt_pad = tgt_pad.to(DEVICE)\n",
    "        \n",
    "        imgs = torch.stack([T(x) for x in imgs])\n",
    "        \n",
    "        logits = net(imgs/255,targets[:-1,:],tgt_pad)\n",
    "        \n",
    "        targets = targets[1:].reshape(-1)\n",
    "        logits = logits.reshape(-1, logits.shape[-1])\n",
    "        \n",
    "        loss = loss_fn(logits,targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses += loss.item()\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    print(\"Starting val for \", epoch)\n",
    "    \n",
    "    net = net.eval()\n",
    "    for p in net.parameters():\n",
    "        p.requires_grad = False\n",
    "        \n",
    "    done_iters = {'tall':False,'square':False,'wide':False}\n",
    "    \n",
    "    val_iters = {}\n",
    "    for res in val_loaders.keys():\n",
    "        val_iters[res] = enumerate(val_loaders[res])\n",
    "    \n",
    "    while not all(done_iters.values()):\n",
    "        \n",
    "        sel = np.random.choice(['tall','square','wide'],p=resolution_dist)\n",
    "        while done_iters[sel]:\n",
    "            sel = np.random.choice(['tall','square','wide'],p=resolution_dist)\n",
    "        \n",
    "        i,(imgs,tgt_pad,targets) = next(val_iters[sel])\n",
    "        \n",
    "        if i == val_lens[sel]-1:\n",
    "            done_iters[sel] = True\n",
    "\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "        tgt_pad = tgt_pad.to(DEVICE)\n",
    "        \n",
    "        logits = net(imgs/255,targets[:-1,:],tgt_pad)\n",
    "        \n",
    "        targets = targets[1:].reshape(-1)\n",
    "        logits = logits.reshape(-1, logits.shape[-1])\n",
    "        \n",
    "        loss = loss_fn(logits,targets)\n",
    "        val_losses += loss.item()\n",
    "        \n",
    "    print(\"Loss for epoch \",epoch,\" = \", losses/sum(train_lens.values()) , ' and ', val_losses/sum(val_lens.values()))\n",
    "    \n",
    "    if val_losses/sum(val_lens.values()) <= min_val_loss:\n",
    "        min_val_loss = val_losses/sum(val_lens.values())\n",
    "        torch.save(net.state_dict(), file_dest+'model_'+str(epoch)+'.pth')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
